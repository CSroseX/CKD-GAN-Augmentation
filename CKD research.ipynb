{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27d2be6e",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 1: SETUP AND IMPORTS\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "!pip install -q imbalanced-learn\n",
    "!pip install -q plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7041e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16, MobileNetV2, EfficientNetV2B0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning & Data Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3718fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fe8dd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6347a54",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 2: KAGGLE DATASET DOWNLOAD\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c91c5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"nazmul0087/ct-kidney-dataset-normal-cyst-tumor-and-stone\")\n",
    "print(\"KaggleHub dataset base path:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71fc27f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def find_kidney_dataset_folder(base_path):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if all(cls in dirs for cls in ['Cyst', 'Normal', 'Stone', 'Tumor']):\n",
    "            print(f\"✅ Found dataset folder at: {root}\")\n",
    "            return root\n",
    "    print(\"❌ Could not find class folders automatically.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4451a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = find_kidney_dataset_folder(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563bdeb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if dataset_path is None:\n",
    "    raise FileNotFoundError(\"Dataset folders (Cyst/Normal/Stone/Tumor) not found.\")\n",
    "else:\n",
    "    expected_classes = ['Cyst', 'Normal', 'Stone', 'Tumor']\n",
    "    for cls in expected_classes:\n",
    "        print(f\"{cls} folder exists:\", os.path.exists(os.path.join(dataset_path, cls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf15eb",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 3: DATA LOADING AND PREPROCESSING\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793bd92a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b059a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, base_path, img_size=(128, 128)):\n",
    "        self.base_path = base_path\n",
    "        self.img_size = img_size\n",
    "        self.classes = ['Cyst', 'Normal', 'Stone', 'Tumor']\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "    def load_images(self):\n",
    "        \"\"\"Load and preprocess images from dataset folders\"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(self.base_path, class_name)\n",
    "            if not os.path.exists(class_path):\n",
    "                print(f\"Warning: Path {class_path} not found\")\n",
    "                continue\n",
    "\n",
    "            image_files = [f for f in os.listdir(class_path)\n",
    "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            print(f\"Loading {len(image_files)} images from {class_name}...\")\n",
    "\n",
    "            for img_file in image_files:\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('L')      # grayscale\n",
    "                    img = img.resize(self.img_size)               # 128×128\n",
    "                    img_array = np.array(img, dtype=np.float32)\n",
    "                    img_array = (img_array - 127.5) / 127.5       # normalize to [-1, 1]\n",
    "                    images.append(img_array)\n",
    "                    labels.append(self.class_to_idx[class_name])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {img_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        images = np.expand_dims(np.array(images), axis=-1)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        print(f\"\\nTotal images loaded: {len(images)}\")\n",
    "        print(f\"Image shape: {images.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        return images, labels\n",
    "\n",
    "    def get_class_distribution(self, labels):\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        return dict(zip([self.classes[i] for i in unique], counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset_path)\n",
    "X_original, y_original = data_loader.load_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be930911",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class_dist = data_loader.get_class_distribution(y_original)\n",
    "print(\"\\nOriginal Class Distribution:\")\n",
    "for cls, count in class_dist.items():\n",
    "    print(f\"  {cls}: {count}\")\n",
    "# Expected: Cyst: 3709 | Normal: 5077 | Stone: 1377 | Tumor: 2283 | Total: 12446"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aeae1b",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 4: CLASS IMBALANCE VISUALIZATION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83e336",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_class_distribution(labels, classes, title=\"Class Distribution\"):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    class_names = [classes[i] for i in unique]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "\n",
    "    ax1.bar(class_names, counts, color=colors)\n",
    "    ax1.set_xlabel('Class', fontsize=12)\n",
    "    ax1.set_ylabel('Number of Images', fontsize=12)\n",
    "    ax1.set_title(title, fontsize=14)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    for i, (name, count) in enumerate(zip(class_names, counts)):\n",
    "        ax1.text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "    ax2.pie(counts, labels=class_names, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    ax2.set_title('Class Proportion', fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('class_distribution_original.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_class_distribution(y_original, data_loader.classes,\n",
    "                             \"Original Dataset Class Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65d841",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 5: DIMENSIONALITY REDUCTION (t-SNE & PCA)\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d108701",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_tsne_pca(X, y, classes, title_prefix=\"Original\"):\n",
    "    X_flat = X.reshape(X.shape[0], -1)\n",
    "\n",
    "    if len(X_flat) > 5000:\n",
    "        indices = np.random.choice(len(X_flat), 5000, replace=False)\n",
    "        X_sample = X_flat[indices]\n",
    "        y_sample = y[indices]\n",
    "    else:\n",
    "        X_sample = X_flat\n",
    "        y_sample = y\n",
    "\n",
    "    print(f\"Computing t-SNE for {title_prefix} data...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "    print(f\"Computing PCA for {title_prefix} data...\")\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_sample)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        mask = y_sample == idx\n",
    "        ax1.scatter(X_tsne[mask, 0], X_tsne[mask, 1],\n",
    "                    c=colors[idx], label=class_name, alpha=0.6, s=20)\n",
    "    ax1.set_title(f't-SNE Visualization - {title_prefix}', fontsize=14)\n",
    "    ax1.set_xlabel('Component 1')\n",
    "    ax1.set_ylabel('Component 2')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        mask = y_sample == idx\n",
    "        ax2.scatter(X_pca[mask, 0], X_pca[mask, 1],\n",
    "                    c=colors[idx], label=class_name, alpha=0.6, s=20)\n",
    "    ax2.set_title(f'PCA Visualization - {title_prefix}', fontsize=14)\n",
    "    ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "    ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'tsne_pca_{title_prefix.lower().replace(\" \", \"_\")}.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21537c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tsne_pca(X_original, y_original, data_loader.classes, \"Original Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c5b38",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 6: SMOTE AUGMENTATION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(X, y, target_samples):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"APPLYING SMOTE AUGMENTATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    X_flat = X.reshape(X.shape[0], -1)\n",
    "\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    max_samples = max(counts)\n",
    "\n",
    "    # Ensure target samples meet minimum class size requirement\n",
    "    if target_samples < max_samples:\n",
    "        print(f\"Warning: target_samples ({target_samples}) < largest class ({max_samples}).\")\n",
    "        print(f\"Setting target_samples to {max_samples}.\")\n",
    "        target_samples = max_samples\n",
    "\n",
    "    sampling_strategy = {i: target_samples for i in unique}\n",
    "\n",
    "    print(f\"\\nTarget samples per class: {target_samples}\")\n",
    "    print(\"Applying SMOTE...\")\n",
    "\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42, k_neighbors=5)\n",
    "    X_smote_flat, y_smote = smote.fit_resample(X_flat, y)\n",
    "\n",
    "    X_smote = X_smote_flat.reshape(-1, 128, 128, 1)\n",
    "    X_smote = np.clip(X_smote, -1, 1)\n",
    "\n",
    "    print(f\"\\nSMOTE Augmentation Complete!\")\n",
    "    print(f\"Original dataset size: {len(X)}\")\n",
    "    print(f\"SMOTE dataset size: {len(X_smote)}\")\n",
    "\n",
    "    unique, counts = np.unique(y_smote, return_counts=True)\n",
    "    print(\"\\nSMOTE-Augmented Class Distribution:\")\n",
    "    for idx, count in zip(unique, counts):\n",
    "        print(f\"  {data_loader.classes[idx]}: {count}\")\n",
    "\n",
    "    return X_smote, y_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cf4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smote, y_smote = apply_smote(X_original, y_original, target_samples=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c58e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_class_distribution(y_smote, data_loader.classes,\n",
    "                             \"SMOTE-Augmented Class Distribution\")\n",
    "visualize_tsne_pca(X_smote, y_smote, data_loader.classes, \"SMOTE Augmented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73210649",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 7: ACGAN IMPLEMENTATION\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACGAN:\n",
    "    def __init__(self, img_shape=(128, 128, 1), num_classes=4, latent_dim=100):\n",
    "        self.img_shape = img_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.optimizer = keras.optimizers.Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile discriminator first\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(\n",
    "            loss=['binary_crossentropy', 'sparse_categorical_crossentropy'],\n",
    "            optimizer=self.optimizer,\n",
    "            metrics=['accuracy', 'accuracy']\n",
    "        )\n",
    "\n",
    "        # Build generator \n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Build combined model with frozen discriminator\n",
    "        noise = layers.Input(shape=(self.latent_dim,))\n",
    "        label = layers.Input(shape=(1,))\n",
    "        img = self.generator([noise, label])\n",
    "\n",
    "        self.discriminator.trainable = False  \n",
    "        valid, target_label = self.discriminator(img)\n",
    "\n",
    "        self.combined = models.Model([noise, label], [valid, target_label])\n",
    "        self.combined.compile(\n",
    "            loss=['binary_crossentropy', 'sparse_categorical_crossentropy'],\n",
    "            optimizer=self.optimizer\n",
    "        )\n",
    "\n",
    "    def build_generator(self):\n",
    "        \"\"\"Build generator network — architecture unchanged from paper\"\"\"\n",
    "        noise = layers.Input(shape=(self.latent_dim,))\n",
    "        label = layers.Input(shape=(1,), dtype='int32')\n",
    "\n",
    "        label_embedding = layers.Embedding(self.num_classes, self.latent_dim)(label)\n",
    "        label_embedding = layers.Flatten()(label_embedding)\n",
    "        model_input = layers.Multiply()([noise, label_embedding])\n",
    "\n",
    "        x = layers.Dense(16 * 16 * 128)(model_input)\n",
    "        x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = layers.Reshape((16, 16, 128))(x)\n",
    "\n",
    "        x = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "\n",
    "        x = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "\n",
    "        x = layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "\n",
    "        img = layers.Conv2DTranspose(1, kernel_size=4, strides=1, padding='same',\n",
    "                                     activation='tanh')(x)\n",
    "\n",
    "        model = models.Model([noise, label], img, name='generator')\n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        \"\"\"Build discriminator network — architecture unchanged from paper\"\"\"\n",
    "        img = layers.Input(shape=self.img_shape)\n",
    "\n",
    "        x = layers.Conv2D(32, kernel_size=3, strides=2, padding='same')(img)\n",
    "        x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "\n",
    "        x = layers.Conv2D(64, kernel_size=3, strides=2, padding='same')(x)\n",
    "        x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "\n",
    "        x = layers.Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
    "        x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "\n",
    "        x = layers.Conv2D(256, kernel_size=3, strides=1, padding='same')(x)\n",
    "        x = layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "\n",
    "        x = layers.Flatten()(x)\n",
    "\n",
    "        validity = layers.Dense(1, activation='sigmoid', name='validity')(x)\n",
    "        label    = layers.Dense(self.num_classes, activation='softmax', name='label')(x)\n",
    "\n",
    "        model = models.Model(img, [validity, label], name='discriminator')\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=100, batch_size=32, save_interval=10):\n",
    "        \"\"\"Train ACGAN\n",
    "        [paper p.12: \"training occurs over 100 epochs\"] → epochs=100\n",
    "        batch_size not stated in paper → keep 32\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING ACGAN\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake  = np.zeros((batch_size, 1))\n",
    "\n",
    "        history = {'d_loss': [], 'g_loss': []}\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Train Discriminator \n",
    "            idx    = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs   = X_train[idx]\n",
    "            labels = y_train[idx]\n",
    "\n",
    "            noise          = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            sampled_labels = np.random.randint(0, self.num_classes, batch_size).reshape(-1, 1)\n",
    "            gen_imgs       = self.generator.predict([noise, sampled_labels], verbose=0)\n",
    "\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, [valid, labels])\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs,\n",
    "                                                             [fake, sampled_labels.flatten()])\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train Generator \n",
    "            noise          = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            sampled_labels = np.random.randint(0, self.num_classes, batch_size).reshape(-1, 1)\n",
    "\n",
    "            g_loss = self.combined.train_on_batch([noise, sampled_labels],\n",
    "                                                   [valid, sampled_labels.flatten()])\n",
    "\n",
    "            history['d_loss'].append(d_loss[0])\n",
    "            history['g_loss'].append(g_loss[0])\n",
    "\n",
    "            if epoch % save_interval == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs} - D Loss: {d_loss[0]:.4f}, \"\n",
    "                      f\"D Acc: {100*d_loss[3]:.2f}%, G Loss: {g_loss[0]:.4f}\")\n",
    "\n",
    "        print(\"\\nACGAN Training Complete!\")\n",
    "        return history\n",
    "\n",
    "    def generate_images_topup(self, class_counts, target_per_class=7000):\n",
    "        \"\"\"\n",
    "        [FIX-4] Generate exactly enough synthetic images so that each class\n",
    "        reaches target_per_class when combined with original images.\n",
    "\n",
    "        Paper p.12: \"increasing the total number of images per class up to 7,000,\n",
    "        which results in a balanced dataset of 28,000 unique images\"\n",
    "\n",
    "        class_counts: dict {class_idx: original_count}\n",
    "        target_per_class: 7000\n",
    "\n",
    "        Returns: (X_synthetic, y_synthetic)\n",
    "          Cyst (0):   7000 - 3709 = 3291 synthetic images\n",
    "          Normal (1): 7000 - 5077 = 1923 synthetic images\n",
    "          Stone (2):  7000 - 1377 = 5623 synthetic images\n",
    "          Tumor (3):  7000 - 2283 = 4717 synthetic images\n",
    "          Total synthetic: 15,554\n",
    "          Total combined:  28,000\n",
    "        \"\"\"\n",
    "        print(f\"\\nGenerating per-class top-up synthetic images (target: {target_per_class}/class)...\")\n",
    "\n",
    "        generated_images = []\n",
    "        generated_labels = []\n",
    "\n",
    "        for class_idx in range(self.num_classes):\n",
    "            orig_count = class_counts[class_idx]\n",
    "            needed     = target_per_class - orig_count\n",
    "\n",
    "            if needed <= 0:\n",
    "                print(f\"  Class {class_idx}: already has {orig_count} images, no generation needed.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Class {class_idx} ({data_loader.classes[class_idx]}): \"\n",
    "                  f\"{orig_count} original → generating {needed} synthetic...\")\n",
    "\n",
    "            noise  = np.random.normal(0, 1, (needed, self.latent_dim))\n",
    "            labels = np.full((needed, 1), class_idx)\n",
    "\n",
    "            imgs = self.generator.predict([noise, labels], verbose=0)\n",
    "            generated_images.append(imgs)\n",
    "            generated_labels.extend([class_idx] * needed)\n",
    "\n",
    "        X_synthetic = np.vstack(generated_images)\n",
    "        y_synthetic = np.array(generated_labels)\n",
    "\n",
    "        print(f\"\\nTotal synthetic images generated: {len(X_synthetic)}\")\n",
    "        print(f\"Expected combined total: {len(X_synthetic) + sum(class_counts.values())}\")\n",
    "        return X_synthetic, y_synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2016efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train ACGAN\n",
    "acgan = ACGAN(img_shape=(128, 128, 1), num_classes=4, latent_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerator Summary:\")\n",
    "acgan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b4042",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDiscriminator Summary:\")\n",
    "acgan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e837cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = acgan.train(X_original, y_original, epochs=100, batch_size=32, save_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d212306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['d_loss'], label='Discriminator Loss', alpha=0.7)\n",
    "plt.plot(history['g_loss'], label='Generator Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('ACGAN Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('acgan_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original counts from loaded data\n",
    "original_class_counts = {\n",
    "    0: int(np.sum(y_original == 0)),   # Cyst:   3709\n",
    "    1: int(np.sum(y_original == 1)),   # Normal: 5077\n",
    "    2: int(np.sum(y_original == 2)),   # Stone:  1377\n",
    "    3: int(np.sum(y_original == 3)),   # Tumor:  2283\n",
    "}\n",
    "print(\"\\nOriginal class counts:\", original_class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca5556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_synthetic, y_synthetic = acgan.generate_images_topup(\n",
    "    class_counts=original_class_counts,\n",
    "    target_per_class=7000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSynthetic dataset shape: {X_synthetic.shape}\")\n",
    "print(f\"Synthetic labels shape:  {y_synthetic.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce637678",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 8: VISUALIZE SYNTHETIC IMAGES\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9361bf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_synthetic_samples(generator, classes, num_samples=4):\n",
    "    fig, axes = plt.subplots(len(classes), num_samples, figsize=(12, 10))\n",
    "\n",
    "    for class_idx, class_name in enumerate(classes):\n",
    "        noise  = np.random.normal(0, 1, (num_samples, 100))\n",
    "        labels = np.full((num_samples, 1), class_idx)\n",
    "\n",
    "        gen_imgs = generator.predict([noise, labels], verbose=0)\n",
    "        gen_imgs = (gen_imgs + 1) / 2.0 \n",
    "\n",
    "        for i in range(num_samples):\n",
    "            axes[class_idx, i].imshow(gen_imgs[i, :, :, 0], cmap='gray')\n",
    "            axes[class_idx, i].axis('off')\n",
    "            if i == 0:\n",
    "                axes[class_idx, i].set_title(class_name, fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('synthetic_samples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af6bfb4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "visualize_synthetic_samples(acgan.generator, data_loader.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac5e96",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 9: CREATE BALANCED COMBINED DATASET\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38734e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = np.vstack([X_original, X_synthetic])\n",
    "y_combined = np.concatenate([y_original, y_synthetic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e139d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMBINED DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(X_combined)}\")   # Expected: 28,000\n",
    "print(f\"Shape: {X_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify per-class counts\n",
    "unique, counts = np.unique(y_combined, return_counts=True)\n",
    "print(\"\\nCombined Class Distribution:\")\n",
    "for idx, count in zip(unique, counts):\n",
    "    print(f\"  {data_loader.classes[idx]}: {count}\")  # Each should be 7,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a0a09",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "visualize_class_distribution(y_combined, data_loader.classes,\n",
    "                             \"Combined Dataset (Original + Synthetic) — 28,000 images\")\n",
    "visualize_tsne_pca(X_combined, y_combined, data_loader.classes, \"Combined Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d26ad",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 9.5: QUANTITATIVE REALISM METRICS (FID & SSIM)\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556caf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from scipy.linalg import sqrtm\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e49329",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c3ab5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def scale_images_for_inception(images):\n",
    "    \"\"\"Convert (N,128,128,1) [-1,1] → (N,299,299,3) InceptionV3-preprocessed\"\"\"\n",
    "    images = (images + 1) * 127.5                                   # [-1,1] → [0,255]\n",
    "    images = tf.image.grayscale_to_rgb(\n",
    "        tf.constant(images.astype(np.uint8), dtype=tf.float32))     # 1ch → 3ch\n",
    "    images = tf.image.resize(images, (299, 299)).numpy()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3508f90",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_fid(model, images1, images2):\n",
    "    img1 = preprocess_input(scale_images_for_inception(images1))\n",
    "    img2 = preprocess_input(scale_images_for_inception(images2))\n",
    "\n",
    "    act1 = model.predict(img1, verbose=0)\n",
    "    act2 = model.predict(img2, verbose=0)\n",
    "\n",
    "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
    "\n",
    "    ssdiff  = np.sum((mu1 - mu2) ** 2)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e903cd08",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_ssim(images1, images2, data_range=2.0):\n",
    "    sample_size = min(len(images1), len(images2), 1000)\n",
    "    idx1 = np.random.choice(len(images1), sample_size, replace=False)\n",
    "    idx2 = np.random.choice(len(images2), sample_size, replace=False)\n",
    "\n",
    "    ssim_scores = [\n",
    "        ssim(images1[idx1[i], :, :, 0], images2[idx2[i], :, :, 0], data_range=data_range)\n",
    "        for i in range(sample_size)\n",
    "    ]\n",
    "    return np.mean(ssim_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55dcccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CALCULATING REALISM METRICS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22514306",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_fid = min(len(X_original), len(X_synthetic))\n",
    "print(f\"\\nCalculating FID using {sample_size_fid} real and {sample_size_fid} synthetic images...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_score = calculate_fid(\n",
    "    inception_model,\n",
    "    X_original[np.random.choice(len(X_original), sample_size_fid, replace=False)],\n",
    "    X_synthetic[np.random.choice(len(X_synthetic), sample_size_fid, replace=False)]\n",
    ")\n",
    "print(f\"Fréchet Inception Distance (FID): {fid_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c38ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCalculating SSIM between sampled real and synthetic images...\")\n",
    "ssim_score = calculate_ssim(X_original, X_synthetic)\n",
    "print(f\"Average Structural Similarity Index (SSIM): {ssim_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b239a0b",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 10: TRANSFER LEARNING MODELS\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_model(base_model_name, input_shape, num_classes):\n",
    "    \"\"\"Build transfer learning model with correct preprocessing per architecture\"\"\"\n",
    "    input_layer = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Convert grayscale → 3-channel for all models\n",
    "    x = layers.Concatenate()([input_layer, input_layer, input_layer])\n",
    "\n",
    "    if base_model_name == 'EfficientNetV2':\n",
    "        x = layers.Rescaling(scale=127.5, offset=127.5)(x)\n",
    "        base_model = EfficientNetV2B0(weights='imagenet', include_top=False,\n",
    "                                      input_shape=(128, 128, 3))\n",
    "    elif base_model_name == 'VGG16':\n",
    "        base_model = VGG16(weights='imagenet', include_top=False,\n",
    "                           input_shape=(128, 128, 3))\n",
    "    elif base_model_name == 'MobileNetV2':\n",
    "        base_model = MobileNetV2(weights='imagenet', include_top=False,\n",
    "                                 input_shape=(128, 128, 3))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {base_model_name}\")\n",
    "\n",
    "    # Phase 1: freeze entire base model for warm-up\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x      = base_model(x)\n",
    "    x      = layers.GlobalAveragePooling2D()(x)\n",
    "    x      = layers.Dense(256, activation='relu')(x)\n",
    "    x      = layers.Dropout(0.5)(x)\n",
    "    x      = layers.Dense(128, activation='relu')(x)\n",
    "    x      = layers.Dropout(0.3)(x)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=input_layer, outputs=output, name=base_model_name)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_finetuning(model, base_model, X_train, y_train, X_val, y_val,\n",
    "                          model_name, warmup_epochs=10, finetune_epochs=20):\n",
    "    \"\"\"\n",
    "    Two-phase training:\n",
    "      Phase 1 (warm-up): train only the custom head with base frozen\n",
    "      Phase 2 (fine-tune): unfreeze last 20 layers, train at lower LR\n",
    "    Paper: \"fine-tuned independently on both original and ACGAN-augmented dataset\"\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    callbacks_warmup = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7),\n",
    "        ModelCheckpoint(f'{model_name}_warmup_best.keras', monitor='val_accuracy',\n",
    "                        save_best_only=True, mode='max')\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nPhase 1: Warm-up ({warmup_epochs} epochs, base model frozen)\")\n",
    "    history_warmup = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=warmup_epochs,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks_warmup,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Phase 2: Unfreeze last 20 layers of base model for fine-tuning\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False  # keep all layers except last 20 frozen\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    callbacks_finetune = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-8),\n",
    "        ModelCheckpoint(f'{model_name}_best.keras', monitor='val_accuracy',\n",
    "                        save_best_only=True, mode='max')\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nPhase 2: Fine-tuning ({finetune_epochs} epochs, last 20 layers unfrozen, LR=1e-5)\")\n",
    "    history_finetune = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=finetune_epochs,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks_finetune,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Merge histories for plotting\n",
    "    combined_history = {\n",
    "        'accuracy':     history_warmup.history['accuracy']     + history_finetune.history['accuracy'],\n",
    "        'val_accuracy': history_warmup.history['val_accuracy'] + history_finetune.history['val_accuracy'],\n",
    "        'loss':         history_warmup.history['loss']         + history_finetune.history['loss'],\n",
    "        'val_loss':     history_warmup.history['val_loss']     + history_finetune.history['val_loss'],\n",
    "    }\n",
    "    return combined_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8465c2",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 11: DATA SPLITTING\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3737ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA SPLITTING — 70% Train / 15% Val / 15% Test (Stratified)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bde10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: split off 30% for val + test\n",
    "X_train_orig, X_temp, y_train_orig, y_temp = train_test_split(\n",
    "    X_original, y_original,\n",
    "    test_size=0.30, random_state=42, stratify=y_original\n",
    ")\n",
    "# Step 2: split the 30% equally into 15% val and 15% test\n",
    "X_val_orig, X_test_orig, y_val_orig, y_test_orig = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.50, random_state=42, stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad4d53",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"\\nOriginal data splits:\")\n",
    "print(f\"  Train: {X_train_orig.shape}  ({len(X_train_orig)/len(X_original)*100:.1f}%)\")\n",
    "print(f\"  Val:   {X_val_orig.shape}   ({len(X_val_orig)/len(X_original)*100:.1f}%)\")\n",
    "print(f\"  Test:  {X_test_orig.shape}  ({len(X_test_orig)/len(X_original)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c661ad",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 11: TRAIN ON ORIGINAL DATASET\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a97b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING ON ORIGINAL DATASET\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_original    = {}\n",
    "histories_original = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f639cd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for model_name in ['VGG16', 'MobileNetV2', 'EfficientNetV2']:\n",
    "    model, base_model = build_transfer_model(model_name, (128, 128, 1), 4)\n",
    "    history = train_with_finetuning(\n",
    "        model, base_model,\n",
    "        X_train_orig, y_train_orig,\n",
    "        X_val_orig, y_val_orig,\n",
    "        f'{model_name}_original'\n",
    "    )\n",
    "    models_original[model_name]    = model\n",
    "    histories_original[model_name] = history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c8d427",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 12: TRAIN ON AUGMENTED DATASET\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f69eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING ON AUGMENTED DATASET\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05374e2b",
   "metadata": {},
   "source": [
    "Apply the same 70/15/15 split to the combined dataset —\n",
    "but only the TRAINING portion uses synthetic images.\n",
    "Val and test are identical to original splits (untouched)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cdd500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build augmented training set: original train + ALL synthetic\n",
    "X_train_aug = np.vstack([X_train_orig, X_synthetic])\n",
    "y_train_aug  = np.concatenate([y_train_orig, y_synthetic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1033c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "shuffle_idx  = np.random.permutation(len(X_train_aug))\n",
    "X_train_aug  = X_train_aug[shuffle_idx]\n",
    "y_train_aug  = y_train_aug[shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f6534",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nAugmented training set: {X_train_aug.shape}\")\n",
    "print(f\"Validation set (original only): {X_val_orig.shape}\")\n",
    "print(f\"Test set (original only):       {X_test_orig.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_augmented    = {}\n",
    "histories_augmented = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fafd07d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for model_name in ['VGG16', 'MobileNetV2', 'EfficientNetV2']:\n",
    "    model, base_model = build_transfer_model(model_name, (128, 128, 1), 4)\n",
    "    history = train_with_finetuning(\n",
    "        model, base_model,\n",
    "        X_train_aug, y_train_aug,\n",
    "        X_val_orig, y_val_orig,\n",
    "        f'{model_name}_augmented'\n",
    "    )\n",
    "    models_augmented[model_name]    = model\n",
    "    histories_augmented[model_name] = history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117f0691",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 13: EVALUATION - Val + Test sets\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a68b1c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                              accuracy_score, roc_auc_score, roc_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c90f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title_prefix):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['accuracy'],     label='train_acc')\n",
    "    plt.plot(history['val_accuracy'], label='val_acc')\n",
    "    plt.title(f'{title_prefix} - Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['loss'],     label='train_loss')\n",
    "    plt.plot(history['val_loss'], label='val_loss')\n",
    "    plt.title(f'{title_prefix} - Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title_prefix.replace(\" \", \"_\").lower()}_training_plot.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_eval, y_eval, classes, model_name, split_name='val'):\n",
    "    print(f\"\\nEvaluating {model_name} on {split_name} set...\")\n",
    "\n",
    "    y_pred_prob = model.predict(X_eval, verbose=0)\n",
    "    y_pred      = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_eval, y_pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_eval, y_pred, target_names=classes))\n",
    "\n",
    "    # AUC-ROC (one-vs-rest, per class + macro)\n",
    "    try:\n",
    "        auc_macro = roc_auc_score(\n",
    "            tf.keras.utils.to_categorical(y_eval, num_classes=len(classes)),\n",
    "            y_pred_prob,\n",
    "            multi_class='ovr', average='macro'\n",
    "        )\n",
    "        print(f\"Macro AUC-ROC: {auc_macro:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"AUC-ROC could not be computed: {e}\")\n",
    "        auc_macro = None\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_eval, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix — {model_name} ({split_name})')\n",
    "    plt.savefig(f'confusion_matrix_{model_name}_{split_name}.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return {'accuracy': acc, 'auc_macro': auc_macro, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbe282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on validation AND test sets\n",
    "evaluation_results = {'original': {}, 'augmented': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0080e0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for model_name in ['VGG16', 'MobileNetV2', 'EfficientNetV2']:\n",
    "    # Original models\n",
    "    model_orig = models_original[model_name]\n",
    "    hist_orig  = histories_original[model_name]\n",
    "    plot_training_history(hist_orig, f'{model_name} (Original)')\n",
    "\n",
    "    res_val  = evaluate_model(model_orig, X_val_orig,  y_val_orig,\n",
    "                              data_loader.classes, model_name, 'val_original')\n",
    "    res_test = evaluate_model(model_orig, X_test_orig, y_test_orig,\n",
    "                              data_loader.classes, model_name, 'test_original')\n",
    "    evaluation_results['original'][model_name] = {\n",
    "        'val': res_val, 'test': res_test\n",
    "    }\n",
    "    model_orig.save(f'{model_name}_original_model.keras')\n",
    "    print(f\"Saved: {model_name}_original_model.keras\")\n",
    "\n",
    "    # Augmented models\n",
    "    model_aug = models_augmented[model_name]\n",
    "    hist_aug  = histories_augmented[model_name]\n",
    "    plot_training_history(hist_aug, f'{model_name} (Augmented)')\n",
    "\n",
    "    res_val  = evaluate_model(model_aug, X_val_orig,  y_val_orig,\n",
    "                              data_loader.classes, model_name, 'val_augmented')\n",
    "    res_test = evaluate_model(model_aug, X_test_orig, y_test_orig,\n",
    "                              data_loader.classes, model_name, 'test_augmented')\n",
    "    evaluation_results['augmented'][model_name] = {\n",
    "        'val': res_val, 'test': res_test\n",
    "    }\n",
    "    model_aug.save(f'{model_name}_augmented_model.keras')\n",
    "    print(f\"Saved: {model_name}_augmented_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a03d079",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 14: COMPARISON SUMMARY TABLE\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6673219",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "for model_name in ['VGG16', 'MobileNetV2', 'EfficientNetV2']:\n",
    "    orig_val_acc  = evaluation_results['original'][model_name]['val']['accuracy']\n",
    "    orig_test_acc = evaluation_results['original'][model_name]['test']['accuracy']\n",
    "    aug_val_acc   = evaluation_results['augmented'][model_name]['val']['accuracy']\n",
    "    aug_test_acc  = evaluation_results['augmented'][model_name]['test']['accuracy']\n",
    "\n",
    "    summary_rows.append({\n",
    "        'Model':                    model_name,\n",
    "        'Original Val Acc':         f\"{orig_val_acc:.4f}\",\n",
    "        'Original Test Acc':        f\"{orig_test_acc:.4f}\",\n",
    "        'Augmented Val Acc':        f\"{aug_val_acc:.4f}\",\n",
    "        'Augmented Test Acc':       f\"{aug_test_acc:.4f}\",\n",
    "        'Paper Original Target':    {'VGG16': '99.2%', 'MobileNetV2': '97.2%', 'EfficientNetV2': '97.62%'}[model_name],\n",
    "        'Paper Augmented Target':   {'VGG16': '97.3%', 'MobileNetV2': '95.1%', 'EfficientNetV2': '97.0%'}[model_name],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e8226",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print('\\n' + '='*70)\n",
    "print('MODEL COMPARISON SUMMARY vs. PAPER TARGETS')\n",
    "print('='*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "summary_df.to_csv('model_comparison_summary.csv', index=False)\n",
    "print('\\nSaved: model_comparison_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7992ad",
   "metadata": {},
   "source": [
    "============================================================================\n",
    "SECTION 15: CLEANUP\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nAll done. Output files:')\n",
    "print('  - class_distribution_original.png')\n",
    "print('  - tsne_pca_*.png')\n",
    "print('  - acgan_training_history.png')\n",
    "print('  - synthetic_samples.png')\n",
    "print('  - confusion_matrix_*.png')\n",
    "print('  - *_training_plot.png')\n",
    "print('  - *.keras  (saved models)')\n",
    "print('  - model_comparison_summary.csv')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
